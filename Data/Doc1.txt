RAG stands for Retrieval Augmented Generation.
It combines a retriever and a generator.
The retriever searches documents and provides context.
The generator uses the context to produce better answers.
Local LLMs can be used together with indexing libraries like LlamaIndex.
LLMs stands for Large language models.

Why use RAG:
- reduces hallucination
- lets you answer from private/company docs
- can update knowledge by changing documents (no retraining)


LlamaIndex:
- loads documents
- chunks them
- builds an index
- provides query_engine to retrieve + generate